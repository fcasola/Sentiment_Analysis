[TrainingData]
# dataset - local path to the positive and negative examples
# files can be downloaded from:
# https://www.kaggle.com/bittlingmayer/amazonreviews
## the first training dataset contains 3.6M reviews for training
dataset_train_path: ../data/Training/train.ft.txt.bz2
## the test dataset contains 400K reviews for training
dataset_test_path: ../data/Training/test.ft.txt.bz2
# Specify the #of reviews from the training and test dataset to be retained
N_max_train_test: [2e4,1e4]

[VecRep]
# defining word representation
# decide whether you want to use a dense or a 1-hot representation
# the dense representation (strongly suggested!) is the Google Word2Vec pretrained dictionary
Use_dense_rep: True
# If Word2Vec is not used,
# decide whether you want to count words using tfIdf (prediction not yet implemented) or simple 1-hot
tfIdf: False        
# url link to the file containing pretrained word2vec representation (if needed, it will download it)
website_pretrained_Word2vec: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
# local destination folder to download the Word2Vec representation (if needed), or path where to find the file (if already present)
Dest_fold_dwl = E:/ 

[LocalDest]
# defining net and training parameters
# CPU/GPU platform on which the training should be running
device_name = /cpu:0
# path and filename where the training model will be saved
pathgraph = ../data/model/ 
# prefix for and destination for saving the trained model
path_model = ../data/model/model_nn

[TrainingPars]
# Define the net geometry: dimension of each layer
# Dimension of the hidden layer: specify how deep (# of elements in the list) and how
# wide (value of the elements in the list). If empty, the model will coincide with logistic-regression
Dim_hidden: []
# Define the learning rate and other parameters
learning_rate: 0.04
epochs: 20
batch_size: 128   
